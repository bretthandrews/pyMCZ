\documentclass{emulateapj} 
\usepackage{amsmath}
\usepackage{float}
%\usepackage{deluxetable}
\usepackage{natbib}
\usepackage{hyperref}

%\usepackage{csvsimple}

\newcommand{\kms}{\ensuremath{{\rm km~s}^{-1}}}
\newcommand{\oxab}{\ensuremath{12 + \log_{10}(\frac{O}{H})}}
\newcommand{\ha}{H$\alpha$}
\newcommand{\hb}{H$\beta$}
%\usepackage[toc\input{Metallicity_MCuncertainties.tex}
%,page]{appendix}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{verbatim}
%\usepackage{graphicx}
%\usepackage{epsfig}
%\usepackage{morefloats}
%\usepackage{float}
%\usepackage{lipsum}
%\usepackage{subfigure}
%\usepackage{longtable}
%\usepackage{lipsum} 
%\usepackage{rotating}
%\usepackage{wasysym}

\begin{document}
\title{Monte Carlo Method for Calculating Uncertainty in Oxygen Abundance from Strong-Line Flux Measurements}

\author{Author order to be determined: Maryam Modjaz\altaffilmark{1}, Federica B. Bianco\altaffilmark{1}, Seung Man Oh\altaffilmark{1,2}, David Fierroz\altaffilmark{1}, Yuqian Liu\altaffilmark{1}, Lisa Kewley\altaffilmark{3,4}}
\altaffiltext{1}{Center for Cosmology and Particle Physics, New York University, 4 Washington Place, New York, NY 10003, USA}
 \altaffiltext{2}{NYU Abu Dhabi PO Box 129188 Abu Dhabi, UAE}
 \altaffiltext{3}{Australian National University, Research School for Astronomy \& Astrophysics, Mount Stromlo Observatory, Cotter Road, Weston, ACT 2611, Australia }
 \altaffiltext{4}{ Institute of Astronomy, University of Hawaii, 2680 Woodlawn Drive, Honolulu, HI 96822, USA}
 
 
\begin{abstract}
\textbf{MODIFY \& FINALIZE AT THE END:} We present an open-source Python code for the determination of the strong-emission-line estimators of oxygen abundance in the standard scales, based on the original IDL-code in \citet{kewley02}. The standard strong line Metallicity scales and diagnostics \text{IMPROVE} have been used to estimate metal abundance through emission line ratios. Here we introduce a Monte Carlo resampling of these methods in order to better characterize an oxygen abundance confidence region.  We output median values, 16th and 84th percentile confidence regions, for various standard metallicity diagnostics, and, when possible, for reddening E(B-V). We produce Monte Carlo parameter distributions for the oxygen abundance and when possible for reddening E(B-V). We test our code on emission lines measurements from a sample of galaxies ($z<0.15$) and compare our metallicity results with those from previous methods. We show that our metallicity estimates are consistent with previous methods but yields smaller uncertainties. The code is open source and can be found at \url{www.github.com/nyusngroup/} \textbf{
(add repo and DOI)}.
\end{abstract}
 
\section{Introduction}
%FED: i ove the opening sentence! however i think syntactically you cannot refer to ``the low quantity of something'' unless you have mentioned it before. so I suggest ``Small amounts of'' instead of The low quantity of
Small amounts of carbon, oxygen, nitrogen, sulfur and iron and
%among the change goes with the initial change
 other elements provide a splash of color to the otherwise dominating greyscape of hydrogen and helium in the stars and gas of galaxies. Nevertheless, even this minute presence of heavy elements (all elements heavier than H and He, also called metals or collectively metallicity) is important for many areas of astrophysics. For example, \citet{johnson12}, amongst others, 
%FED: among others (this is a very common theory i think)
suggest that if it was not for the relatively high metallicity level in our Solar System, planet formation may not have been possible. With $Z$ representing the mass fraction of metals, for our own Sun the value is measured to be  Z=0.0153 \citep{chaffau11}, though there are others who suggest a lower solar metallicity of $Z=0.0134$ mostly because of oxygen \citep{asplund09_rev,grevesse10}.%\footnote{Note that these abundances refer to the current abundances in the Sun, which are lower than the value with which the Sun was formed 4.56 Gyr ago, since diffusion at the bottom of the convection zone has decreased metallicity over time \citep{grevesse10}.%FED: too much detail perhaps?}. 
Furthermore, when properly observed and estimated, metallicity measurements of galaxies can tightly constrain models of galaxy formation and evolution (e.g., \citealt{kewley08} and references therein), as well as shed light on the metallicity dependence and production conditions for different types of Supernovae (SNe) and long-duration Gamma-Ray Bursts (GRBs) (e.g., \citealt{modjaz08_Z,levesque10_grbhosts,anderson10,modjaz11,kelly12,sanders12,lunnan14,leloudas14,pan14}).

%chaffau Z=0.0209 12+log=O/H) = 8.76¬±0.07 
%Small variations in chemical abundance can often be critical indicators for stellar and galactic properties like age and activity.
%FED: I  quite like the following paragraph. I realize it is a lot of detail but maybe we can include up to ``passing generation''?
Metals are produced in the cores of massive stars during their fusion life cycle but also during the extreme conditions of stellar explosions. For example, the majority of iron is synthesized in thermonuclear explosions (SNe Ia) while nearly all of oxygen and other $\alpha$-elements are released in core collapse SNe (SNe Ib, Ic, \& II). Since new stars are born from the clouds these explosions enrich, metallicity will increase with each passing generation of stars. 

%Besides age, metallicity also influences stellar temperature. Metals are more effective at absorbing energy coming from the interior of the star in the stellar atmosphere so their presence increases the stellar opacity. With greater absorption and opacity the radius expands to a size larger than it would be without metals and this larger size results in a cooler effective temperature. While metallicity can help indicate age or activity it's temperature and density dependencies require parameterization to properly be estimated.

However, for almost all astronomical objects, metallicity cannot be measured directly. The oxygen abundance in the gas-phase is the canonical choice of metallicity indicator for interstellar medium (ISM) studies, since oxygen is the most abundant metal and only weakly depleted onto dust grains (in contrast to refractory elements such as Mg, Si, Fe, with Fe, being depleted by more than a factor of 10 in Orion; see \citealt{simondiaz11-orion}). The oxygen abundance\footnote{We note that in many cases in the literature, including here, the terms metallicity and oxygen abundance are used interchangeably.} is expressed as  \oxab, where $O$ and $H$ represent the number of Oxygen and Hydrogen atoms, respectively. In particular, \citep{chaffau11} measure a solar oxygen abundance of \oxab = 8.76 $\pm$ 0.07, while \citet{asplund09_rev} suggest \oxab = 8.69.

%FED: why is Oxygen capital here but not elsewhere? 
Importantly, oxygen exhibits very strong nebular lines in the optical wavelength range of HII regions (e.g., \citealt{pagel79,osterbrock89,tremonti04}), which can be measured. 
%FED: this is not clear: how does the developing of diagnostics using various lines follow from strong O lines??  
Thus, many different diagnostic techniques, relying on different lines of oxygen, hydrogen and other elements, have been developed (e.g., \citealt{kewley02,pettini04,kobulnicky04,kewley08}), which are discussed in the next section. Ultimately the  purpose of this paper is to support the release of a public code that computes metallicity according to the standard abundance diagnostics as well as the associated uncertainties due to the measured emission line flux uncertainties.

%FED: the introduction should definitely at some point state what this paper is about, and you haven't yet. thus i suggest not splitting the introduction into two paragraphs. And i would mention earlier that the purpose of this paper is to support the release of a public code to compute metallicity according to various scales.

\subsection{The different oxygen abundance diagnostics}

Here we present a brief overview of the various observational methods for measuring the gas-phase oxygen abundance - however, for a full discussion with all the caveats we encourage the reader to read the reviews by e.g. \citet{stasinska02,kewley08,moustakas10,stasinska10,dopita13,blanc15}.
The so-called ``classical'' way to estimate the oxygen abundance is the electron temperature ($T_e$) method, which estimates the electron temperature and density of the nebula using a number of oxygen lines with different ionization states, including the auroral [OIII] $\lambda$4363 line\footnote{Note however, that most recently \citet{berg15} suggest that [OIII] $\lambda$4363 line is the most problematic auroral line to use amongst those of [OII], [OIII] [NII], [SII], [SIII], giving rise to temperature discrepancies.}, to then directly estimate the OII and OIII abundances to obtain the total oxygen abundance, after correcting for the unseen stages of ionization. However, the auroral [OIII] $\lambda$4363 line is very weak, except in low-metallicity environments, and saturates at higher metallicity (since at higher metallicities the cooling is dominated by the oxygen NIR fine structure lines) $-$ thus, other methods had to be developed that use other, stronger lines in the spectra of HII regions. These are called \emph{strong-line methods} and are the subject of this manuscript. Strong-line methods can be categorized into two types: theoretical methods, that rely on calibrating various observed line ratios using photoionization methods (basically theoretically simulating HII regions, using stellar model atmospheres, stellar population synthesis and photoionization models) and empirical ones, that calibrate various observed strong line ratios using observed $T_e$-based metallicities. While historically there have been large systematic offsets between the $T_e$ method and the strong line methods, \citet{dopita13} demonstrated that the $T_e$ method gives the same results as the strong line methods, if the energy distribution of the electrons in the HII regions is assumed to not be a simple Maxwell-Boltzmann distribution (as assumed in prior works), but a more realistic $\kappa$ distribution, as observed in solar system astrophysical plasma. They also find that the effect of the $\kappa$ distribution on the strong-line methods is minor. 

%For strong-line methods, there are again two classes of methods: those that calibrate the metallicity based on theoretical model grids that include stellar population plus photoionization (called "theoretical methods"), and those that that are empirical which use $Te$-measured metallicities for calibration (called "empirical methods"). 
For theoretical strong-line method, a ratio of oxygen line fluxes to $H\beta$, referred to as $R_{23}$, is commonly used to determine the metallicity of galaxies  \citep{pagel79}:
%FED i think we should move this out of inline for redeability. 
$$R_{23}=\frac{\mathrm{[OII]} \lambda 3727~+~\mathrm{[OIII]} \lambda 4959,\lambda 5007}{\mathrm{H}_\beta},$$ 
where [OIII]$\lambda$4959,$\lambda$5007 stands for the sum of the two [OIII] lines
%FED: you have not explained yet what [OIII] $\lambda 4959,\lambda 5007$ means, i.e. with the comma between \lambda 4959 & \lambda 5007
 The drawback of this method is that it is double-valued with metallicity, and thus other line ratios need to be used to break the degeneracy between the high values ("upper branch") and the low values ("lower branch") of the $R_{23}$ metallicities (e.g., \citealt{kewley08}). 
%can we reference a specific ratio on any paper, if not including a figure here, to clarify this double valued business?
Furthermore, \citet{kewley02} showed the importance of ionization parameter, which can be physically understood as corresponding to the maximum velocity of an ionized front that can be driven by the local radiation field of hot massive stars that are ionizing the ISM gas. This ionization parameter needs to be taken into account in the various strong-line methods, as HII regions at the same metallicity but with different ionization parameters produce different line strengths. Calibrations of $R_{23}$ by \citet{mcgaugh91} (hereafter M91), by \citet{kewley02} (hereafter KD02), and by \citet{dopita13} (hereafter D13) use different theoretical photoionization models and take the ionization parameter into account, while other calibrations such as that of \citet{zaritsky94} (hereafter Z94) do not. Thus, Z94 is mostly valid for only metal-rich galaxies.  M91 and KD02 use an iterative process to break the $R_{23}$ degeneracy  (KD02 uses different ratios [NII]$/$[OII] and [NII]/\ha) and to also constrain the ionization parameter $q$ in order to arrive at the metallicity estimate.

%The most direct way to estimate metallicity in spectra is to measure line fluxes absorbed or emitted by metals and non-metals. While iron lines may be present in stellar spectra, iron is less commonly found in gaseous nebulae than oxygen is. Not only is oxygen more abundant but it emits several strong lines ([OI] $\lambda 6300$, [OII] $\lambda 3727,7318,7324$, [OIII] $\lambda 4363, 4959, 5007$) visible at optical wavelengths that can also be used to quantify temperature and density. For this reason 
%While the ratio of oxygen to hydrogen line flux correlate with metallicity, the ratio of two [OIII] lines, one auroral $\lambda 4363$ and the other an excitation line at $\lambda 5007$ can be used to determine what is called the Ionization Correction Factor (ICF) (Kewley \& Dopita 2002).
As to empirical strong-line methods, the most commonly used ones are that by \citet{pettini04} (hereafter PP04) and \citet{pilyugin05}. PP04 used HII regions with measured $T_e$-based metallicities to derive empirical fits to strong-line ratios, and introduced the line ratios of ([NII]/\hb ($N2$) and ([OIII]$/$\hb)$/$([NII]$/$\ha ($O3N2$) as metallicity diagnostics. Since PP04\_N2 employs two closely spaced lines (\ha and NII), which are not affected by stellar absorption, nor (uncertain) reddening, and are easily observed in one simple spectroscopic setup, it has become an often-used scale, at least for low-z SN host galaxy studies (see meta-analysis by e.g,. \citealt{sanders12,modjaz12_proc,leloudas14}). However, it is important to remember that this scale has a number of short-comings: it does not take into account the impact of the ionization parameter, it was initially derived based on only 137 extragalactic HII regions, and the nitrogen emission line employed saturates at high metallicity, and thus the PP04\_N2 method saturated for high-metallicity galaxies (at $\oxab > 8.8$, \citealt{kewley08}). An updated calibration by \citet{marino13} based on many more $T_e$-based metallicities (almost three time larger that that of PP04) derives a significantly shallower slope between $O3N2$ index and oxygen abundance than the PP04 calibration. In addition, most recently, \citet{berg15} suggest that the auroral [OIII] $\lambda$4363 line, commonly used for $T_e$ measurements, is the most problematic auroral line to use amongst those of [OII], [OIII] [NII], [SII], [SIII], giving rise to temperature discrepancies.


As it can be seen, each scale has different advantages and disadvantages and should be used in different metallicity regimes (see detailed discussion in e.g.,  \citealt{kewley02,stasinska02,kewley08,moustakas10,dopita13,blanc15}). Thus, this open-source code outputs the oxygen abundance in the main 7\footnote{as of version v1.0, Spring 2015} metallicity scales (with the KD02 diagnostic having four outputs and the PP04 diagnostic having two outputs). While there is a long-standing debate about which diagnostic to use, as there are systematic metallicity offsets between different methods (recombination lines vs. strong-line method vs. ``direct'' $T_e$ method, see the above sources), \emph{\bf the relative metallicity trends can be considered robust, if the analysis is performed self-consistently in the same scale, and trends are seen across different scales \citep{kewley08,moustakas10}}. Thus, it is then necessary to obtain statistical error bars for relative comparisons that are meaningful. Note however, that while there are conversion values between different scales \citep{kewley08}, they apply for large data sets, since those conversion values were derived based on ten thousands of SDSS galaxies, and thus should be used with caution (or not at all) for smaller samples
%FED this is not strictly correct: you do want to derive conversions from large samples, but what you are saying is that you do not know about the spread of the conversion distribution, and thus you do not know how much individual data points could deviate. Is that not stated in the papers that derive the conversions??.
In addition, one should note that there is a debate about the value of the solar oxygen abundance \citep{asplund09_rev,chaffau11}, such that the absolute oxygen calibration is still uncertain.



Here we introduce the open-source python code "\textbf{FILL IN}". pro .. In \S~\ref{method_sec} we describe our method, and the input and output values of the code. In \S~\ref{comp_sec}, we compare our method of obtaining abundance uncertainties to previous methods in the literature. 

%\begin{figure}[H]
%\epsscale{1.0} 
%\begin{center}
%\includegraphics[width=0.89\columnwidth]{fig1.png} 
%\caption{Example of a sampled Gaussian. 50,000 points from a Gaussian distribution were selected in this case}
%\label{f1}
%\end{center}
%\end{figure}


\section{Description of Metallicity code}\label{method_sec}


\begin{figure*}[!ht]
%\epsscale{0.6}
\centerline{
\includegraphics[width=0.98\columnwidth]{exampledata_n2000_EB-V_1.pdf}
\includegraphics[width=0.98\columnwidth]{exampledata_n2000_M91_1.pdf}}
% exampledata_n50000_M91_1.ps}
%\vspace{0.2in}
\centerline{
\includegraphics[width=0.98\columnwidth]{exampledata_n2000_PP04_O3N2_1.pdf}
\includegraphics[width=0.98\columnwidth]{exampledata_n2000_KK04comb_1.pdf}}
\caption{Metallicity and reddening E(B-V) parameter distributions based on the example data shown in~\ref{tab:exampledata}: emission line data of the HII regions at the position of SN~2008D, published in \citet{modjaz11}. The distribution are generated from $N$=2,0000 samples. The median values are shown with the dashed lines, while the region between the 16th and the 84th percentile is shaded (orange). We show the metallicity scaled from \citet{kobulnicky04}, updated as described in  \citet{kewley08} (KK04\_comb), \citet{pettini04}, using OIII and NII  (PP04 O3N2) and \citet{mcgaugh91} (M91). Similar plots are outputted by the code for each metallicity scale calculated. Each plot indicates: the scale, the sequential number of measurement in input, corresponding to a line of the input file, the median, 16th and 84th percentile values, and the method to choose the bin size for the histogram (Kuth's rule in this case, see Section~\ref{vizs}).}
 \label{metallicity_distribution}
%\end{center}
%\vspace{-0.5in}
\end{figure*}


For computing oxygen abundances, we use the iterative code by \citet{kewley02}, which has been updated in \citet{kewley08} and reflects ... \textbf{LISA: YOUR INPUT HERE:what is the update if any or is it exactly as in Kewley \& Ellison 08?? } which was initially written in IDL.  We translated the code into python, and added new features, most importantly the capability of obtaining uncertainties on the metallicity outputs via Monte Carlo resampling, and made it open source on GitHub, as we explain below.


\subsection{Input and Output of code}

The input of the code is a set of spectral emission line fluxes. We assume that the observed emission lines to be used to indicate metallicity originate in HII regions and are not due to non-thermal excitation by e.g., AGN or interstellar shocks from SNe or stellar winds.  Tests to exclude data contaminated by such non-thermal sources should be executed using the recommended line ratios by e.g., \citealt{baldwin81,kauffmann03,kewley06_sdss} \emph{prior to running this code}. Furthermore these lines should have all the correct calibration (at least correct relative calibration) and \emph{should have a signal-to-noise ratio (S/N) of at least 3}. The latter is important for the success of the Monte Carlo resampling technique as described below.

Emission line flux values are fed into our Python implementation as in the original IDL code by \citet{kewley02}, hereafter referred to as IDLKD02. The inputs are emission line flux values and their uncertainties for the following lines: \ha, \hb, [OI] 6300, [OII] 3727, [OIII] 4959, [OIII] 5007, [NII] 6584, [SII] 6717, [SII] 6731.  [SIII] 9096 and SIII 9532 can be used to calculate $S_{23}$, but are not often observed since they are in the NIR and thus, are currently not used to calculate metallicity with any of the diagnostics enabled by this code. The line fluxes are to be stored in an ASCII file, and the measurement errors in a separate ASCII files.
%FED: should they be [SIII] instead of SIII?
(consult the README.md\footnote{\url{https://github.com/nyusngroup/MC_Metalicity/blob/master/README.md}} in the GitHub repository for details about the input format, and sample files). \textbf{CHECK AT THE END WITH CODE!} 
 If the fluxes for the specified lines are not available, the entry should be set to `NaN' and the out-putted oxygen abundance will be calculated only for metallicity scales that use the provided line fluxes. In absence of measurement errors the flux errors entry should be set to 0.0 (and the code will not generate confidence intervals, as explained later in this Section).

As part of the code, the inputted line fluxes are corrected for reddening by using the observed Balmer decrement, for which \ha~ and  \hb~ flux values need to be provided. We assume case B recombination, and thus the standard value of 2.86 as the
intrinsic \ha/\hb~ ratio \citep{osterbrock89}, and apply the standard Galactic reddening law with $R_V$ = 3.1 \citep{cardelli89}. However, the user can choose other extinction laws and $R_V$ values, if desired, given the code's open-source nature. If the input measurements are already de-reddened, the user can easily disable the reddening correction. If either \ha~ or \hb~ are not provided the reddening correction cannot be implemented, and thus, the user is notified and has the option to proceed with the calculation with uncorrected line fluxes.

As output, we obtain metallicity values and their uncertainties in various metallicity scales. The user can choose which of the following calibrations to calculate, which have all have been implemented as prescribed in \citet{kewley08}, except where noted.
\begin{itemize}
\item {\bf M91} \citep{mcgaugh91}
% based on $R_{23}$. To break the $R_{23}$ degeneracy we follow Appendix 2 in \citet{kewley08}, and use the value of [NII]/[OII].
\item {\bf Z94} \citep{zaritsky94} which is valid for the upper branch of $R_{23}$ only, and we conservatively constrain it to log($R_{23})<0.9$, i.e., the range that is covered by the photoionization model grids.
\item {\bf C01}: a diagnostic based on $R_{23}$, C01\_$R_{23}$, and one based on [NII]/[SII], C01\_N2S2  \citep{charlot01}, \textbf{CHECK}%# - which is however deprecated)  MM: I think we should remove it from the paper and the code
\item {\bf D02} \citep{denicolo02} for which we include, in addition to the uncertainties in the measurements, the uncertainty on the fit parameters published in D02.
\item {\bf PP04}:  \citep{pettini04} 2 computations based on the [NII]/\ha~ ratio, called PP04\_N2, and another based on  ([OIII]/\hb) / [NII]/\ha~), called PP04\_O3N2.  
\item {\bf P05} \citep{pilyugin05}
%, $R_{23}$ based method that is calibrated via $T_e$ metallicities of a sample of HII regions. We use the values of [NII]/[OII], and  [NII]/Ha to discriminate between upper and lower branch 
\item {\bf KD02 and KK04}: 4 computations using $R_{23}$ (KD02\_$R_{23}$, which is KK04 \textbf{CHECK PLEASE - YES RIGHT MM - NEED TO RESTRUCTURE}), the [NII]/[OII] ratio (KD02\_N2O2), the [NII]/\ha~ ratio (KK04\_N2\ha), and a combined method that chooses the optimal method given the input line fluxes (KD comb KK04\_$R_{23}$, KDcomb\_new) (\citealt{kewley02}, \citealt{kewley08}). \textbf{FINISH CHECK}

\item {\bf D13}: Recently, \citet{dopita13} has updated the photoionization models used in KD02 and in KK04 by including new atomic data within a modified photoionization code and by no longer assuming that the energy distribution of the electrons in the HII regions  is a simple Maxwell-Boltzmann distribution (as assumed in prior works), but a more realistic $\kappa$ distribution, as observed in solar system astrophysical plasma \citep{nicholls12}. If the user has installed their publicly available 
%FBBcode redundant with module
\verb=pyqz=\footnote{\url{https://datacommons.anu.edu.au:8443/DataCommons/item/anudc:5037}} Python module, [NII], [SII], [OIII], \ha, and \hb~ lines are fed to the \verb=pyqz= module, which produces up to 8 emission line ratio diagnostics for 12+log(O/H), each using two of the line ratios [NII]/[SII], [NII]/\ha, [OIII]/[SII], and [OIII]/\hb. Our code sets the $\kappa$ parameter to 20, which is the value that \citet{dopita13} found best resolves the inconsistencies between oxygen abundance values derived from the strong-line methods vs. the ``direct'' $T_e$ method \footnote{The user can modify the value of $\kappa$ by editing the code, if they wish.}.
%%MM: we need to look at the D13 outputs since they are not necessarily a different "scale"
% The \verb=pyqz= module assumes a $\kappa$-distribution of energy for the electrons in HII regions, as suggested by \citet{nicholls12}.
\item{\bf DP00 \& P01 (deprecated)}: (\citealt{diaz00} and \citealt{pilyugin01}, respectively) are also available, but deprecated: P01 is superseded by P05. Therefore they are not part of the default output; however, they are available upon explicit user request, via the command line options. 
\end{itemize}

All of the above calibrations, except those of D13 (i.e., \verb=pyqz=) and of DP00, are calculated in the original IDLKD02 code, and are discussed in detail in \citet{kewley02,kewley08}. 
DP00 is the only diagnostic that relies on sulfur ratios: $ S_{23}= {\rm ( [SII]6717+[SIII]9069) } / {\rm H}\beta $ . The shortcomings of $S_{23}$ as a tool to measure abundances are discussed in \citet{kewley02}, and include the fact that it is double-valued for all metallicities, that it depends on the ionization parameter, and that the sulfur-to-oxygen ratio is poorly determined and difficult to model due to large ionization correction factors that are needed to account for the presence of unobserved ionization states. The DP00 diagnostic we implement is corrected with the addition of a term $\propto (c+S_{23}^3)^{-1}$ as suggested by \citep{kewley02}, which corrects the tendency of the scale to systematically underestimates the abundance, with a discrepancy growing larger at higher metallicity. However, we point out the scatter in the metallicity derived from this diagnostic compared to others remain high. 

%P01 \citep{pilyugin01} is a $R_{23}$-based diagnostic, which was improved in \citet{pilyugin05}. We provide tools to calculate the original diagnostic for historical reasons, since it was available in IDLKD02, but its use is deprecated, as the updated Pi05 diagnostic should be used in its place.

If the line fluxes necessary for specific scales are not provided, the output metallicities will default to `NaN'. If the errors in the measurements are not provided, the code will specify that it cannot create a measurement distribution and determine a confidence interval, but it will calculate the nominal metallicity and output it.

The distributions of E(B-V), $\log(R_{23})$ are outputted, together with those of the oxygen abundance in the various scales. While certain parameters, such as the ionization parameter $q$ and the electron density (using the SiII lines) are computed, as long as the necessary lines are provided, they are not outputted in the current version of our code $-$ however, the reader can easily modify the code to suite their needs, given it is an open-source code.



\subsection{Computing Uncertainties}
The novel aspect of our work is that for every set of input line measurements we introduce a Monte Carlo (MC) resampling method to obtain iterations via random sampling within the measurement errors, and thus we obtain a robust result for error estimation (e.g., \citealt{efron79,hastie09,andrae10}). 

Given a data set with error bars from which certain parameters are estimated, Monte Carlo resampling generates synthetic data samples drawing from a given distribution. 
Here we draw synthetic data from a Gaussian distribution centered on each measured line flux value, with standard deviation corresponding to the measurement error. The implicit assumption is made, of course, that the line flux error is Gaussian distributed in nature\footnote{Users may wish to provides their own probability distribution for the emission line uncertainties, and modify the code to suite their needs.}. 

% FED: because we vectorize, we actually do not do this sample by sample so we cannot say ``Every iteration''
For each metallicity scale, for each of $N$  values chosen randomly within the relevant emission line distributions we run the calculations that computes the metallicity.
This effectively simulates conducting multiple experiments when repeating observations is impractical or impossible, as in the case of the emission line flux data, and thus generates alternative data sets. 
%FED redundant: We generated $N$ Monte Carlo flux samples for each emission line, and calculated the metallicity for each set of line fluxes. 
The sample size $N$ is set by the user, and one should expect an appropriate value of $N$ to be a few 1000s, depending on the metallicity scale chosen and measurement errors (for example  $N=2,000$ is determined to be sufficient for our example data, as shown in Section~\ref{sec:completeness}, and we provide tools to assure the sample size is sufficiently large, which we describe in Section~\ref{sec:completeness}). 
%FED: note that 2000 is ok for our sample but it does not need to be! it depends on the error size, and the shape of the distribution, so it depends on the input. if for example the distribution 
% , FED: because we vectorize, we actually do not do this sample by sample, so we cannot say ``At the end of $N$ iterations''
A distribution of parameter estimates for the oxygen abundance is generated for each scale, from which the median metallicity and its confidence region are calculated,
%We bin to visualize, but we calculate percentiles on the actual distribution
and the results are binned and visualized in a histogram (see Section~\ref{vizs})\footnote{However, note this method is a conservative approach, since it overestimates the intrinsic metallicity uncertainty, as we are centering this error distribution on the measured values instead of the (unknown) true values \citep{andrae10}.
%FED i do not know what that means. i would think that this method is NOT conservative on account of that!
}. This is done for each scale the user chooses to calculate. The fiftieth (50\%) percentile, i.e.  the median, is reported, as well as the 16th and 84th percentiles of the metallicity estimate distribution as its confidence region. However, the user can choose to also output the full metallicity parameter distribution as ASCII files, in addition to the plots. %FED redundant? The output of our code includes the oxygen abundance measurements plus confidence regions in all scales, as well as plots of the distributions of the oxygen abundance value in all scales. 

This MC resampling approach takes into account the impact of the uncertain reddening (due to the uncertainties in the measurement of the 
\ha~ and \hb~ fluxes), when the option for de-reddened metallicities is chosen. For each synthetic set of measurements a new reddening value is calculated based on the resampled \ha~ and \hb~ fluxes, and used to compute the de-reddened metallicity value, thus the derived distribution of metallicity values takes into account the uncertain redding. As part of the output, a parameter estimate distribution plot for E(B-V) is provided as well (first panel in figure~\ref{metallicity_distribution}), along with confidence intervals derived using the same method as for the metallicity measurements. If either the \ha~ or the \hb~ flux is not provided, no reddening correction can be applied. The computed metallicity will not be reddening-corrected and the E(B-V) output will be set to zero.

Figure \ref{metallicity_distribution} shows the metallicity estimate distribution for three representative scales (M91, PP04\_O3N2, and KD02comb), and for the reddening parameter E(B-V) - similar plots that are out-putted by our code for all scales as listed above (not all shown here). Although the input distributions are Gaussian, the metallicity distributions 
%FED: in log space they are NEVER gonna be gaussian so not ``rarely are'',
are not, for two reasons: first, since the metallicities are computed based on log values of line flux ratios, symmetric error bars in linear space will translate into asymmetric error bars in log space; and second, some metallicity scale computations are non-linear, and sometimes bimodal, especially those that include $R_{23}$), since they choose upper vs lower branch to break the degeneracy.

Since the metallicity distributions are not Gaussian, the percentiles we report cannot be expressed in terms of $\sigma$ values. In determining the confidence region for asymmetric and multi-modal distributions, there are broadly three approaches (e.g., \citealt{andrae10}): choosing a symmetric interval, the shortest interval, or a \emph{central} interval.  With the central method we determine the  confidence interval by choosing the left and right boundaries such that the region outside the confidence interval on each side contains $16\%$ of the total distribution - in analogy to the one-sigma-interval of a Gaussian distribution. This ensures that the algorithm finds the proper boundaries even for asymmetric, non-Gaussian distributions, and in the case of multiple peaks. In summary, the output for the measured value corresponds to the fiftieth (50\%) percentile, while the lower error bar corresponds to the 50$^{th}$-16$^{th}$ percentile and the upper error bar corresponds to 84$^{th}$-50$^{th}$ of the metallicity estimate distribution. 
%However, we urge the reader to always inspect the metallicity distribution plots, which are also outputted, to check for themselves whether the outputted median and confidence regions properly represent the metallicity distribution (see Section~\ref{vizs})

The distributions for the D02 scale include the uncertainty in the fit parameters: the oxygen abundance in this scale is generated as \oxab~=~$9.12~\pm 0.05 + (0.73~\pm 0.10)~\mathrm{NII}$ \citep{denicolo02}. The parameters of the fit are generated as the sum of the nominal parameters (9.12 and 0.73) and a Gaussian distributed random value centered on zero, and within a standard deviation of 0.05 and 0.10, respectively, in the above units.

We note that our code does not output the \emph{systematic} uncertainty of each scale, which, for example, is $\sim$0.15 dex for KD02. However, if all metallicity measurements are in the \emph{same} scale and only \emph{relative} comparisons are made, as recommended by a number of authors, then the systematic error does not have any impact (by definition!). Nevertheless, it is then necessary to obtain statistical error bars, as computed via the code we are releasing here, for meaningful relative comparisons.

%However, the
%relative metallicity difference measured between a given pair
%of galaxies in different diagnostics is consistent with an rms
%scatter typically ?0.07 dex, and 0.15 dex between the most
%discrepant diagnostics (Kewley&Ellison 2008).


%from Kewley & Ellison08: The cause of the metallicity calibration discrepancies remains unclear. The discrepancy has been attributed to either an unknown problem with the photoionization models (Kennicutt et al. 2003) or temperature gradients or fluctuations that may cause metallicities based on the electron temperature method to underestimate the true metallicities (Stasin¬´ska 2002, 2005; Bresolin 2006). Until this discrepancy is resolved, the absolute metallicity scale is uncertain.

\subsubsection{Visual diagnostics}\label{vizs}
In order for the user to check the validity of a measurement, and to better understand the distribution, we provide two visualizations: for each set of input line fluxes, we generate a histogram of the output distribution in all metallicity scales (Figure \ref{metallicity_distribution} and \ref{fig:KDE}), and for each set of input line fluxes we generate a \emph{box-and-whiskers} plot (hereafter \emph{boxplot}, for short) summarizing the result of all scales calculated (Figure \ref{boxplot}). 

Choosing the binning size for a histogram is not a trivial task.  \citet{hogg08} describes various data analysis recipes for selecting a histogram bin size. Too many bins will result in many empty bins and an "over-fit'' histogram, while too few bins may miss features of the distribution. 
By default, we use \emph{Knuth's Method} to choose the number of bins $N_\mathrm{bins}$ for each histogram. Knuth's method  optimizes a Bayesian fitness function across fixed-width bins \citep{knuth06}. Additionally, however, we enable a number of binning options from which the user can choose, including: 
the square root of the number of bins, \emph{Rice rule} ($N_\mathrm{bins}~=~2\sqrt[3]{N}$, e.g., \citealt{hastie09}), 
\emph{Doane's formula} ($N_\mathrm{bins}~=~1 + \mathrm{log}_2{N} + \mathrm{log}_2\left(1 + \mathrm{Kurt}\sqrt{(N / 6)}\right)$, where Kurt is the third moment of the distribution, \citealt{doane76}\footnote{\citet{doane76} attempted to address the issue of finding the proper number of bins for the histogram of a skewed distribution. Several versions of the so-called Doane's formula can be found in the literature. Our formula can be found, for example, in \citealt{bonate11}}), and the full Bayesian solution, known as Bayesian Blocks, which optimizes a fitness function across an arbitrary configuration of bins, such that the bins are of variable size \citep{scargle13}. The implementation of the latter method requires the \verb=astroML= python package to be installed on the user's system (\citealt{astroml}\footnote{\url{https://github.com/astroML/astroML}}). If the \verb=astroML= package is not found, the code will default to Knuth's Rule. 
As mentioned, Knuth's method implies an optimization. In cases in which the convergence of this minimization takes too long (or if the number of bins after the minimization is  $N_\mathrm{bins}/\sqrt{N} > 5$ or $N_\mathrm{bins}/\sqrt{N} < 1/3$) the code will revert to Rice rule.
Some methods may be computationally prohibitive with a very large sample size, or very little computational power, such as the Bayesian methods that that rely on optimization (Knuth's method and the Bayesian Block method) in which case the user may choose to use Doane's formula, Rice rule, or even the square root of the number of samples, to choose the bin size for the histograms. 

\begin{figure}[ht!]
%  \includegraphics[trim = 10mm 0mm 10mm 0mm, clip]{abcomparison6.eps}
  \includegraphics[width=1.0\columnwidth]{exampledata_n2000_PP04_O3N2_1KDE.pdf}
   \caption{The distribution of values for the \citet{pettini04} scale (PP04 O3N2) for the first measurement of our example data (Table~\ref{tab:exampledata}) is plotted. The histogram is generated from the $N=2,000$ samples distribution using Knuth's rule to choose the bin size (Section~\ref{vizs}), and the shaded region represent the Kernel Density estimate for the distribution, calculated via \emph{KD Tree} with a top-hat function. }
\label{fig:KDE}
\end{figure}
\textbf{Fed: please include more info and the motivation for the Kernel density plot}. Lastly, the user can generate and visualize a \emph{Kernel Density} if the \verb=sklearn= package is installed. The Kernel Density of the distribution is then calculated via  \emph{KD Tree} with a top-hat function, as explained in the \verb=sklearn= package documentation\footnote{\url{http://scikit-learn.org/stable/modules/density.html}}. The results will then show both a histogram, with $N_\mathrm{bins}$ chosen via Knuth's method, as well as the distribution Kernel Density, as shown in Figure~\ref{fig:KDE}.


\begin{figure}[ht!]
%  \includegraphics[trim = 10mm 0mm 10mm 0mm, clip]{abcomparison6.eps}
  \includegraphics[width=0.95\columnwidth]{exampledata_boxplot2000_m1.pdf}
   \caption{A box plot shows the comparison of the results of 6 metallicity scales calculated from the same set of measured lines (Table~\ref{tab:exampledata}). For each scale, the median of the resulting distribution is plotted as a horizontal line, the inter quartile range (IQR) is represented as an orange box, and the bars, joined to each end of the box by a dashed line, represent the minimum and maximum of the distribution \emph{excluding outliers}, where outliers are defined as any point farther than $1.5\times$ IQR from the edges for the IQR.}
   %% MM: include the D13 scales
%%the plot gets very busy very quickly, i included the same scales for which we have the histograms in fig 1
 \label{boxplot}
\end{figure}

The boxplot summarizes the result from each scale the user chooses to calculate. For each scale the median of the \oxab~ distribution is plotted as a black horizontal line. The height of the corresponding box represents the $25^\mathrm{\emph{th}}$ percentile of the \oxab~ distribution. 
The bars represent the maximum and minimum value of the distribution, excluding outliers. The outliers, that are plotted as circles, are defined as all data points farther than  $1.5\times\mathrm{IQR}$, where IQR is the the \emph{interquartile range} (and the length of the box) from the $25^\mathrm{\emph{th}}$ percentile (i.e. from either end of the box). 
The Solar oxygen abundance is indicated in this plot for comparison: a gray box shows a range of estimated values for Solar oxygen abundances, from \oxab=8.69 \citep{asplund09_rev} to \oxab=8.76 \citep{chaffau11}.
Notice that only the diagnostics requested by the user have a slot in the plot (in the example in Figure~\ref{fig:KDE} the computed scales are M91, the PP04 scales, and the KD02 scales). However these slot exists on the plot whether the diagnostic can be produced or not, i.e. if the set of input lines does not allow a requested scale to be calculated an empty column will be generated in this plot in correspondence of said metallicity scale.

%Using a bin size of $\sqrt{N}$ was recommended, but this proved to be slightly over-fitting, and after a number of tests, we found $2* \sqrt[3]{N}$ to be appropriate for most cases.

% While the histograms at sufficiently high $N$, where $N$ is the total number of iterations, yielded single peaked results, at lower $N$ there were occasional multiple peaks resulting from a non-smooth gaussian being sampled. In selecting the value of $N$, we found that around $N=20,000$ the KS tests showed the same distributions as larger sized samples, i.e. that the results converged ({\bf FED CHECK MY WORDING!}).  

\subsection{Visual diagnostic to assure completeness of the MC simulation}\label{sec:completeness}

\begin{figure*}[!ht]
%\epsscale{0.6}
\centerline{
\includegraphics[width=0.65\columnwidth]{exampledata_n200_testcomplete.pdf}
\includegraphics[width=0.65\columnwidth]{exampledata_n2000_testcomplete.pdf}
\includegraphics[width=0.65\columnwidth]{exampledata_n20000_testcomplete.pdf}}
% exampledata_n50000_M91_1.ps}
%\vspace{0.2in}
\caption{Cumulative plots of the distribution of metallicity values for the D02 \citep{denicolo02} and KD02 scale (\citealt{kewley02}, updated by \citealt{kewley08}), chosen here just as examples, where $x$ refers to \oxab. The input used is example data 1. In each plot the cumulative distribution is shown for a randomly chosen subsample of 10\% of the data in the sample, as well as a subsample of 25\%, 50\%, and 75\%, of the data, and for all data in the distribution. In the left plots the distributions are generated from an $N=200$ samples, in the center top and bottom plots from an $N=2,000$  and in the right-most column from a $N=20,000$ sample. The increasing overlap of the distributions informs us about completeness. In the left plots the distributions do not fully overlap, indicating that completeness is not achieved with the $N=200$ sample. On the other end, since all subsamples are indistinguishable in the top and bottom plots on the right, which are generated with $N=20,000$ samples, we conclude that completeness is already achieved at 10\% of the $N=20,000$ sample (the smallest sample in the plots on the right) for our example data.}
 \label{cd}
%\end{center}
%\vspace{-0.5in}
\end{figure*}

The user can choose the $N$, number of samples to be generated. Of course the reliability of the metallicity estimates depends crucially on the sample size being sufficiently large to properly characterize the distribution of metallicity values. It is however not trivial to decide when $N$ is sufficiently large. As soon as $N$ is large enough, and the distribution is well characterized, adding synthetic data will not change its shape. Consider a cumulative distribution for a metallicity scale, D02, for example, which has noise from the measurement errors, as well as from the error in the fit parameters, or KD02, which is a non linear combination of the input line flux values. We plot the cumulative distribution for four randomly selected subsamples of the data points in the distribution, of size 10\%, 25\%, 50\%, 75\% of $N$, as well as for the entire distribution. If 1/10th of the points were a sufficiently large Monte Carlo sample, the cumulative distributions would typically appear smooth and, most importantly, they would overlap. In Figure~\ref{cd} we show the these cumulative plots for a distribution generated with $N=200$, $N=2,000$, and $N=20,000$, for D02 and KD02comb. Note that the final sample size is 10\% larger than the user chosen parameter $N$. The sample size is in fact automatically increased by 10\% at the beginning of the run, in order to assure that even if during the calculations some of the output metallicities were to result in non-valid values (`NaN's or infinities, for example, if devision by very small numbers is required) the actual sample size is at least as large as the user intended. With reasonable input parameters, the code rarely produces non-valid values, and if the size of the valid output distribution were in the end smaller than the \emph{requested} value $N$, i.e. if the number of non-valid outputs is larger than 10\% of $N$, the user should in fact worry about the set of input parameters used. 

The cumulative distributions for the subsets of the $N=200$ simulation (left panel in Figure~\ref{cd}) are different, and noisy, indicating that 200 data points is not a sufficiently large dataset. Conversely, at $N=20,000$ (right panel) the distributions are indistinguishable, indicating that $N=2,000$ (the smaller subset plotted) is large enough. The $N=2,000$ cumulative distributions rapidly converge, as the subsample size increases, indicating that a value of $N$  between 200 and 2,000 will characterize the distribution appropriately for our example data set.  In the light of this we choose, conservatively, $N=2,000$ to run our simulations for this dataset. 

We remind the user that the appropriate number of $N$ samples will depend on the diagnostic, and on the input data. The errors on the measurements, and the calculations performed to derive the metallicity from line ratios, which are for many scales non-linear, will determine the shape of the distribution, and thus the number of datapoints that are needed to fully characterize it. 
%FBB rephrased
%MM: Fed I don't understand what you mean by "The way in which the lines are combined" - do you mean how the metallicity is derived based on emission line ratio? The way you have it sounds confusing and out of context (lines of É evidence?)

\subsection{Performance and Benchmarks}
We ran a bench mark calculation on a MacBook Air with an Intel Core i7 (1.7~GHz) and 8GB of 1600 MHz DDR3 memory. The dataset we used for the calculation includes 2 sets of line measurements (measurement 1, or M1, and measurement 2, M2). The flux values and their associated errors are shown in Table~\ref{tab:exampledata}. The code performs simple algebraic operations on large data arrays. It is vectorized along the sample-size dimension, so that the code loops over the smaller dimension corresponding to the number of sets of line measurements in input (2 in our example data), while operations are generally performed on the $N$-dimensional vectors storing the synthetic measurements, and the $N$-dimensional variable hence derived.
%(certain scales, such as the KD02 and KK04 ones, derive the ionization parameter in an iterative fashion requiring further loops ).

%MM: for the last sentence looks like something is missing? While É, and .. doesn't make sense.
%nothing was missing... but i moved the part on iterative calculations at the end. perhaps that is what was confusing?


%real	0m44.877s
%user	0m43.748s
%sys	0m0.420s

\begin{deluxetable*}{lccccccccc} 
%\setlength{\tabcolsep}{0.0001in} 
\tabletypesize{\tiny}
\setlength{\tabcolsep}{0.001in} 
\tablecolumns{10}
%%\tablewidth{114.88pt}
\tablecaption{Example Data and their Uncertainties based on Data in \citet{modjaz11}}
\tablehead{   % column headings
  \colhead{ ~} &
  \colhead{[OII]3727} & 
  \colhead{\hb} & 
  \colhead{[OIII]4959} & 
  \colhead{[OIII]5007} & 
  \colhead{[OI]6300} & 
  \colhead{\ha} & 
  \colhead{[NII]6584} & 
  \colhead{[SII]6717} & 
  \colhead{[SII]6731} 
}
\startdata
M1 & 1.842 (.053) &	0.958 (.032) &	NaN          &	0.302 (.029) &	0.127 (.021) &	4.746 (.026) &	1.642 (.022) &	0.941 (.021) &	0.543 (.019) \\
M2 & 2.875 (.101) &	1.251 (.044) &	0.168 (.028) &	0.064 (.025) &	NaN          & 	4.026 (.069) &	0.781 (.033) &	0.821 (.035) &	0.573 (.032) \\
\enddata
\label{tab:exampledata}
\end{deluxetable*}

With full graphic output (all histograms are being plotted) and performing all default calculations, except those of D13 \emph{pyqz}, for our example data sets and a sample size $N=2,000$, the time required by the code is $\sim43$ seconds (wall clock time), less than 35 seconds of actual CPU time, and less then a second of CPU time spent in the kernel within the process. Including  the scales of D13 \emph{pyqz} for the same datasets, the run time becomes $\sim78$ seconds, with  $\sim65$ seconds of actual CPU time. The code running time and the computational time scale roughly linearly with the number of measurements in input (with of course dependence on which lines are available in each measurement). The code time was tested on sets of 1, 2, 5, 10, 25, 50 and 100 identical measuremente (M1 of our example dataset) and the clock is found to scales with a steep slope of $\sim18$ with the number of measurements, while for the CPU time spent in the kernel we find a very shallow dependency of  of $\sim 0.12$ (this is the actualy computational time: most clock time is in fact spent in input-puout and plotting activities).

%For a single set of line measurements, not including \emph{pyqz}, the time drops to less than 11 seconds of CPU time. 

% MM:  According to the plot it took ~14 seconds of running  for 1 measurement - without or without D13? Those 14 seconds are not mentioned above - perhaps you should mention it explicitly (since you have a plot on it!) after the CPU usage above, in the last sentence.
% FBB: the plot is a place holder until we finalize the version of the code.
The time spent on plotting functions, which includes the calculation of the appropriate number of bins for each scale, is substantial: 1.67 seconds per distribution on average with 14 calls for this data set (one for each metallicity scale, E(B-V), and $R_{23}$). In Table~\ref{tab:time}  we summarize the time spent on each metallicity scale for M1, the first measurement of the sample dataset, for each metallicity scale, and we visualize the run time and memory usage in Figure~\ref{fig:mem}. While the CPU usage is modest, the memory usage can be large, depending on the size of the sample.  The memory usage ramps up quickly, as soon as the line samples are created, 
and remains fairly constant throughout the run.  Figure~\ref{fig:mem} shows the memory used by each function in the code as a function of time for a single set of input lines of our example data (M1). The scales that take longer time require the root finding (e.g. M08), or optimizations which are done iteratively (e.g. KD combined).

\begin{figure}[ht!]
%  \includegraphics[trim = 10mm 0mm 10mm 0mm, clip]{abcomparison6.eps}
  \includegraphics[width=1.0\columnwidth]{memusage.pdf}
   \caption{Memory usage: we plot the square root of the memory usage in Megabites as a function of time for running our code (using $N$=2,000 and all metallicity scales except the D13 \emph{pyqz} ones) on a single set of measured emission lines (Table~\ref{tab:exampledata}, M1). The square root is plotted, instead of the natural value, to enhance visibility.  
Two inserts show zoom-ins of the region where most of the metallicity scales are calculated, since the run time of the code is dominated by the calculation for the KD combined scale, and, after all scales are computed, by plotting routines, including the calculation of the bin size with Knuth's rule. 
Each function call is represented by an opening and closing bracket in the main plot, and by a shaded rectangle in the zoom-in inserts.%, extending for the duration of the function run. 
The calculation of NIIOII, which requires 0.32 seconds, is split between the two inserts. Altogether, the calculation of metallicity in all scales, except those in the D13 \emph{pyqz} scales, takes 5.96 seconds. For technical details about the D13 \emph{pyqz} scales performance we refer the reader to the \emph{pyqz} package.}
 \label{fig:mem}
\end{figure}

%% MM: comments as to figure:
%    -  I would suggest making the x and y axis title much larger and with a larger thickness (see David's figure 6)
%    - units for memory: is it MB or MiB ?
%    - caption: I think you mean Megabytes NOT Megabites ?? or do you mean Megabits? The abbreviation for Megabits is Mb and of Megabyte is MB.
% FBB: i will fix all details it in the final version. The MiB is from the original memory tracking code. i think it is MB (wouldbt make sense to be anything else but i need to check why they use MiB). 
%    - on the middle right part, there is the number "1e-3+1.177" which seem to be floating and not belonging to anything - right?
% FBB: no: that is the factor that multiplies the x ticks. there is no way i can fit the full size tick label in the subplot
%   - let's make sure we understand whether KD02 combined is part due to KK04
%FBB: yes, this plot will be redone w the final version of the code



\begin{deluxetable}{lr} 
%\setlength{\tabcolsep}{0.0001in} 
\tabletypesize{\tiny}
\setlength{\tabcolsep}{0.001in} 
\tablecolumns{2}
%%\tablewidth{114.88pt}
\tablecaption{CPU usage by scale}
\tablehead{   % column headings
  \colhead{Scale} &
  \colhead{Time ($\times 10^{-6}$ sec)} 
}

\startdata
E(B-V)    &   100.1     \\ 
$R_{23}$   &   599.9     \\ 
M91       &   600.1      \\ 
Z94       &   100.1       \\
%C01       &        \\ 
P05       &   399.8     \\ 
D02       &   300.2     \\ 
PP04      &   200.0     \\ 
M08       & 1.720$\times 10^6$ \\
P10       &  5300 \\
M13       &   500.0 \\
pyqz      &   1.127$\times 10^6$  \\ 
KK04 $R_{23}$   & 4500    \\ 
KD02 N2O2 &    500.2     \\ 
KD02 N2Ha &   3200    \\ 
KD combined  &   2800 \\
%diags.initialguess()          &      118.0    \\ 
%   diags.calcNIISII()             &     254.0    \\ 
%8497.5        diags.setNII(raw_lines['[NII]6584'])
%  17077.5        diags.setHab(raw_lines['Ha'],raw_lines['Hb'])
%  25457.0        diags.setOlines(raw_lines['[OII]3727'], raw_lines['[OIII]5007'], raw_lines['[OI]6300'], raw_lines['[OIII]4959'])
%  30478.5        diags.setSII(raw_lines['[SII]6717'],raw_lines['[SII]6731'],raw_lines['[SIII]9069'],raw_lines['[SIII]9532'])
% 378698.0        diags.calcNIIOII()
\enddata
\label{tab:time}
\end{deluxetable}



 




\subsection{Availability}
The source code is published under MIT-licensed on GitHub\footnote{\url{repo URL here}}. At this time the code is released under DOI {\b NUMBER HERE} as version 1.0: {\b NAME HERE}. Project
details, step-wise tutorials, and further information can be found in the module README \footnote{\url{README.md URL}}.
Development is done in Linux and OS X. The package requires standard python packages, such as \verb=numpy=, \verb=scipy=, \verb=pylab=, and additional features are enabled if the packages \verb=astroml=, \verb=skitlearn=, and \verb=pyqz= are installed, but the packages are \emph{not} required. Contact the authors to be included in a mailing list and be notified about critical changes. 




\section{Comparison to prior uncertainty computation and other works }\label{comp_sec}


A previous method for determining the uncertainty in the oxygen abundance (as used in \citealt{modjaz08_Z,kewley10,rupke10,modjaz11}) was an \emph{analytic} approach of propagating the emission-line flux uncertainties: it found the maximum and minimum abundances via maximizing and minimizing, respectively, the various line ratios by adding/subtracting to the measured line values their uncertainties. For comparison we computed the metallicities and their errors in both ways (both analytic and using our current MC resampling method) for 3 representative scales. We plot our results and the residuals in Fig.\ref{comp_anal_MC}, which shows a number of important points: i) The metallicity reported as the 50th percentile of the metallicity parameter distribution from the MC resampling method is completely consistent with the analytically derived metallicity -  well within the respective error bars - and thus, the prior published results still stand (unsurprisingly, since our code, aside for the calculation of the confidence interval, uses the same algorithms developed for IDLKD02). ii) The MC resampling method has smaller error bars than the analytic method, especially for the scales of M91 and KD02. This is easily understandable, since the analytic method assumes the worst-case-scenario, as it basically yields 2 metallicity parameter draws (the "minimum" and "maximum") which are in the tail of the full metallicity probability distribution. However, the MC resampling method is the more appropriate method as it empirically characterizes the full parameter estimation distribution.
%assumes that \textbf{all} emission line measurement have been drawn from the 68\% tail of the error distribution and 

\begin{figure}[ht!]
%  \includegraphics[trim = 10mm 0mm 10mm 0mm, clip]{abcomparison6.eps}
  \includegraphics[width=1.2\columnwidth]{abcomparison6.eps}
   \caption{\textbf{FINISH}.Comparison of metallicity estimation between the analytic method and our Monte Carlo resampling method (top) and their residuals (bottom) for 3 different metallicity scales. Flux measurements come from 19 galaxies previously measured in \citet{modjaz11}. To add asymmetric errors in quadrature we use $\mathrm{residual}_{\rm min}=\sqrt {x_{\rm max}^2 + y_{\rm min}^2}$ and $\mathrm{residual}_{\rm max}=\sqrt {x_{\rm min}^2 + y_{\rm max}^2}$}
 \label{comp_anal_MC}
\end{figure}




%Although the flux probability distribution function is assumed to be symmetric, the metallicity errors are asymmetric because they are derived from the log of minimized and maximized flux ratios.
%Rupke: Errors were propagated primarily using analytic expressions, but for abundance and gradient errors we employed Monte Carlo methods.

\subsection{Comparison with other works}

%Here we perform a literature review of both 


The field of SN host metallicity studies has been rapidly developing as these kinds of studies may be crucial avenues for constraining the progenitor systems of different kinds of explosions - however, a few of the works do not compute errors and others not show how they compute their statistical errors  (e.g., \citealt{anderson10,leloudas11,sanders12,leloudas14}). %While \citet{sanders12} compute the line flux errors from a Markov-Chain Monte Carlo fitting of a gaussian to the emission lines, they only mention in passing that they propagate the line flux uncertainties into the metallicity measurements, but do not describe how {\bf this is a little harsh, right? i think mentioning it without explaining is generally the standard for iid errors -> Maryam: " How would you rephrase it then? " }.

In contrast, the general metallicity field has considered in detail how to estimate the uncertainties in measured metallicities- however, none of those codes are open-source and many of them are for specific scales which were chosen by the authors:  \citet{moustakas10} also use MC resampling to estimate the metallicity uncertainties (in their case using $N$=500 trials and assuming a Gaussian distribution) but only do this for two scales, KK04 and P05. For computing the metallicities of the SDSS star forming galaxies, \citet{tremonti04} fit a combination of stellar population synthesis models and  photoionization models to the observed strong emission lines [OII], $H\beta$, [OIII], $H\alpha$, NII and SII and report the median of the metallicity likelihood distribution as the metallicity estimate, with the width of the distribution giving the 1 $\sigma$ (Gaussian) error. However, this constitutes their own scale (the T04 scale).

In the last stages of preparing this manuscript \citet{blanc15} was published.  \citet{blanc15} employ Bayesian inference for doing something similar to \citet{tremonti04} - they use Bayesian inference to derive the joint and marginalized posterior probability density functions for metallicity $Z$ and ionization parameter $q$ given a set of observed line fluxes and an input photoionization model. They provide a publicly available IDL implementation of their method named $IZI$ (inferring metallicities (Z) and ionization parameters) on the author's web site.


\section{Conclusions}\label{comp_sec}

 \textbf{FINISH}. We hope that this open-access code will be helpful for the many different fields where gas-phase metallicities are important, including in the emerging field of SN and GRB host galaxies, where either it is not described how they got uncertainties or no error bars are computed. Given its public-access nature, the users are free to include any new metallicity diagnostics and modify any parts and assumptions (e.g. that the line fluxes are Gaussian distributed).

\acknowledgements
The Modjaz SNYU group at NYU is supported in parts by the NSF CAREER award AST-1352405 and by NSF award AST-1413260. F. B. Bianco is supported by a \emph{James Arthur Fellowship} at the NYU-Center for Cosmology and Particle Physics and Y. Liu by a \emph{James Arthur Graduate Award}.
This code made use of  several Python Modules, including \verb=Matplotlib= \citep{hunter07}.
Some plots are  produced with public code DOI:10.5281/zenodo.15419 available at \url{https://github.com/fedhere/residuals_pylab}.
This research made use of NASA Astrophysics Data
System; the NASA/IPAC Extragalactic Database (NED), which
is operated by the Jet Propulsion Laboratory, California Institute
of Technology, under contract with the National Aeronautics
and Space Administration.



%\epsscale{1.0}
%\begin{center}
%\csvautotabular{table1.csv}
%\label{t1}
%\caption{Comparison of M-M method with MC method}
%\end{center}
%\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\clearpage
%%%%%%%%%%%%%%%% BIBLIOGRAPHY  %%%%%%%%%%%%%%%%%%%%%%%% 
\bibliographystyle{apj}
\bibliography{Metallicity_MCuncertainties.bib}
%\bibliography{refs}

\appendix

\section{Minimum Code validation}
\textbf{WORK WITH FED - and any other code specific things}
\end{document}

